{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe3417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and ideas from:\n",
    "# https://hami-asmai.medium.com/relationship-extraction-from-any-web-articles-using-spacy-and-jupyter-notebook-in-6-steps-4444ee68763f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd79e1-03b4-4080-8100-51ff6a3143e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764b9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span, Doc\n",
    "\n",
    "from spacy.pipeline import merge_entities, merge_noun_chunks\n",
    "from spacy.symbols import ORTH, POS, NOUN, VERB\n",
    "\n",
    "import urllib.request \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install coreferee\n",
    "# !python3 -m coreferee install en\n",
    "\n",
    "# !python3 -m spacy download en_core_web_lg\n",
    "\n",
    "import coreferee\n",
    "\n",
    "# Merged pipeline\n",
    "nlp_coref = spacy.load('en_core_web_lg')\n",
    "nlp_coref.add_pipe('coreferee')\n",
    "nlp_coref.add_pipe('sentencizer')\n",
    "\n",
    "\n",
    "patterns = [[{\"LOWER\": \"hackerfriendly\"}]]\n",
    "attrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\", \"DEP\": \"nsubj\"}\n",
    "\n",
    "ruler = nlp_coref.get_pipe(\"attribute_ruler\")\n",
    "ruler.add(patterns=patterns, attrs=attrs)\n",
    "\n",
    "nlp_merged = spacy.load('en_core_web_lg')\n",
    "nlp_merged.add_pipe('merge_entities')\n",
    "nlp_merged.add_pipe('merge_noun_chunks')\n",
    "\n",
    "ruler = nlp_merged.get_pipe(\"attribute_ruler\")\n",
    "ruler.add(patterns=patterns, attrs=attrs)\n",
    "\n",
    "\n",
    "nlp = nlp_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0f12f7-a2e3-4a95-8f79-c14816f21cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def referee(doc):\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc):\n",
    "        doc = nlp_coref(doc)\n",
    "        \n",
    "    sent = []\n",
    "    for i, tok in enumerate(doc):\n",
    "        if doc._.coref_chains is None:\n",
    "            sent.append(tok.text)\n",
    "            continue\n",
    "        cr = doc._.coref_chains.resolve(tok)\n",
    "        if cr is None:\n",
    "            sent.append(tok.text)\n",
    "        else:\n",
    "            for word in cr:\n",
    "                sent.append(word.text)\n",
    "\n",
    "    return nlp_coref(Doc(vocab=doc.vocab, words=sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cbdda9a-0060-49f1-969c-658b1c01e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sister has a dog . sister loves dog . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(u'My sister has a dog. She loves him.')\n",
    "print(referee(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71e947c-91fa-4efc-939f-2c0b9a5ee470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sister has a cat . sister loves cat . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(u'My sister has a cat. She loves him.')\n",
    "print(referee(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bebdfde-6165-49da-9e6a-21453c60f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My brother has a cat . brother loves cat . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(u'My brother has a cat. He loves her.')\n",
    "print(referee(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ae602b-9ff3-411c-ba4d-54709b165c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My brother has a dog . brother loves dog . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(u'My brother has a dog. He loves him.')\n",
    "print(referee(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4445a5b-694b-43b8-910a-7026cb2546cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My brother\n",
      "has\n",
      "a dog\n",
      ".\n",
      "brother\n",
      "loves\n",
      "dog\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for tok in nlp_merged(str(referee(doc))):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6469e31-976a-4900-bb15-d7c4d704014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[My brother has a dog., He loves him.]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "491395ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/persyn/interaction/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rob/persyn/interaction/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rob/persyn/interaction/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LinearSVC from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rob/persyn/interaction/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rob/persyn/interaction/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-23 08:54:43,473 loading file /home/rob/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST https://tachikoma1.persyn.io:9200/anna-conversations-v0/_search [status:200 duration:0.655s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-summaries-v0/_search [status:200 duration:0.074s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-entities-v0/_search [status:200 duration:0.064s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-relationships-v0/_search [status:200 duration:0.065s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-opinions-v0/_search [status:200 duration:0.068s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-beliefs-v0/_search [status:200 duration:0.064s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from fastapi.responses import RedirectResponse\n",
    "\n",
    "# Add persyn root to sys.path\n",
    "sys.path.insert(0, '/home/rob/persyn/')\n",
    "sys.path.insert(0, '/home/rob/persyn/interaction')\n",
    "\n",
    "from interaction.interact import Interact\n",
    "\n",
    "# Color logging\n",
    "# from utils.color_logging import log\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PERSYN_CONFIG'] = '/home/rob/persyn/config/anna.yaml'\n",
    "\n",
    "# Bot config\n",
    "from utils.config import load_config\n",
    "\n",
    "interact = Interact(load_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8e78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "service='mastodon'\n",
    "channel='https://mas.to/@annathebot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6230cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries, convo, lts = interact.recall.load(service, channel)\n",
    "# summaries, convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2100d8f8-1168-4e73-9d46-10d2d81c67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST https://tachikoma1.persyn.io:9200/anna-conversations-v0/_search [status:200 duration:0.549s]\n"
     ]
    }
   ],
   "source": [
    "service = 'discord'\n",
    "channel = \"962806111193428028|962806111742877729\"\n",
    "\n",
    "ret = interact.recall.ltm.es.search(\n",
    "    index='anna-conversations-v0', \n",
    "    query={\"term\": {\"channel.keyword\": {\"value\": channel}}},\n",
    "    aggs={\"meh\":{\"terms\" : { \"field\" : \"convo_id.keyword\" }}},\n",
    "    size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c680f72-c25e-4f8f-939d-40cde38d5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_ids = set()\n",
    "for hit in ret['hits']['hits']:\n",
    "    convo_ids.add(hit['_source']['convo_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47262381-11e1-4701-837d-54805b857a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc1cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST https://tachikoma1.persyn.io:9200/anna-conversations-v0/_search [status:200 duration:0.072s]\n",
      "POST https://tachikoma1.persyn.io:9200/anna-summaries-v0/_search [status:200 duration:0.068s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " convo_id: gXgtAXrebxmfQYpSZsCfV9 19 \n",
      " Anna and hackerfriendly discussed a photo of a woman with three legs, and hackerfriendly suggested the name Natasha for her.\n"
     ]
    }
   ],
   "source": [
    "# convo = interact.recall.ltm.get_convo_by_id('oL686bsotQztDoq4p5xqoG')\n",
    "# convo = interact.recall.ltm.get_convo_by_id('PtA5kBjXyisQmyrsLJyMje')\n",
    "# \n",
    "\n",
    "convo = []\n",
    "convo_id = \"gXgtAXrebxmfQYpSZsCfV9\"\n",
    "# convo_id = None\n",
    "\n",
    "convo_id = convo_id or random.choice(list(convo_ids))\n",
    "convo = interact.recall.ltm.get_convo_by_id(convo_id)\n",
    "\n",
    "try:\n",
    "    summary = interact.recall.ltm.es.search(\n",
    "        index='anna-summaries-v0', \n",
    "        query={\"term\": {\"convo_id.keyword\": {\"value\": convo_id}}},\n",
    "        size=1000\n",
    "    )['hits']['hits'][0]['_source']['summary']\n",
    "except IndexError:\n",
    "    summary = '(no summary available)'\n",
    "\n",
    "print('\\n', 'convo_id:', convo_id, len(convo), '\\n', summary)\n",
    "\n",
    "# All summaries from this channel from the beginning of time\n",
    "# interact.recall.load(service, channel, summaries=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9431884",
   "metadata": {},
   "outputs": [],
   "source": [
    "archetypes = [\n",
    "    \"Alice\", \"Bob\", \"Carol\", \"Dave\", \"Eve\", \n",
    "    \"Frank\", \"Gavin\", \"Heidi\", \"Ivan\", \"Judy\", \n",
    "    \"Kaitlin\", \"Larry\", \"Mia\", \n",
    "    \"Natalie\", \"Oliver\", \"Peggy\", \"Quentin\", \"Rupert\", \n",
    "    \"Sophia\", \"Trent\", \"Ursula\", \"Victor\", \"Wanda\", \n",
    "    \"Xavier\", \"Yolanda\", \"Zahara\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "768ba6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_conj(tok):\n",
    "    ''' If tok is a conjunct, return all children that are appositional modifiers '''\n",
    "    ret = []\n",
    "    for child in tok.children:\n",
    "        if child.dep_ == 'conj':\n",
    "            ret = [c.text for c in child.children if c.dep_ == 'appos']\n",
    "            if not ret:\n",
    "                ret = [child.text] + find_all_conj(child)\n",
    "    return ret\n",
    "\n",
    "def find_all_pobj(tok):\n",
    "    ''' If tok is an object of a preposition, return all children that are appositional modifiers '''\n",
    "    ret = []\n",
    "    for child in tok.children:\n",
    "        if child.dep_ == 'pobj':\n",
    "            ret = [c.text for c in child.children if c.dep_ == 'appos']\n",
    "            if not ret:\n",
    "                ret = [child.text] + find_all_conj(child)\n",
    "    return ret\n",
    "\n",
    "def find_all_singletons(tok):\n",
    "    ''' Return a list of all descendants with only one child. '''\n",
    "    ret = []\n",
    "\n",
    "    def all_singletons(node):\n",
    "        ok = True\n",
    "        \n",
    "        if len(list(node.children)) > 1:\n",
    "            return False\n",
    "        \n",
    "        for child in node.children:\n",
    "            ok = all_singletons(child)\n",
    "            if not ok:\n",
    "                return ok\n",
    "\n",
    "        return ok\n",
    "            \n",
    "    if not all_singletons(tok):\n",
    "        return ret\n",
    "    \n",
    "    for child in tok.children:\n",
    "        ret = [child.text] + find_all_singletons(child)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_relationships(doc, render=False):\n",
    "    checked = []\n",
    "    clauses = []\n",
    "    ret = {\n",
    "        'left': [],\n",
    "        'rel': [],\n",
    "        'right': []\n",
    "    }\n",
    "    subj = []\n",
    "    root = None\n",
    "\n",
    "    # print(\"ORIG:\", doc)\n",
    "    \n",
    "    doc = nlp_merged(str(doc))\n",
    "\n",
    "    # print('MERGED:', doc)\n",
    "    \n",
    "    # Resolve coreferences\n",
    "    doc = referee(doc)\n",
    "    \n",
    "    # print('RESOLVED:', doc)\n",
    "\n",
    "    if render:\n",
    "        displacy.render(doc)\n",
    "        print(doc)\n",
    "    \n",
    "    for tok in doc:\n",
    "\n",
    "        # Find the ROOT\n",
    "        if tok.dep_ != 'ROOT':\n",
    "            continue\n",
    "\n",
    "        if tok.pos_ not in ['VERB', 'AUX']:\n",
    "            print(\"Root is not a verb, can't continue.\", tok)\n",
    "            return []\n",
    "\n",
    "        ret['rel'] = tok.lemma_.lower()\n",
    "\n",
    "        if not tok.children:\n",
    "            return [ret]\n",
    "\n",
    "        for child in tok.children:\n",
    "            # Include modifiers (if any)\n",
    "            if child.dep_ == 'neg':\n",
    "                ret['rel'] = f\"not {ret['rel']}\"\n",
    "            if child.dep_ == 'advmod':\n",
    "                ret['rel'] = f\"{ret['rel']} {child.text}\"\n",
    "        \n",
    "        for child in tok.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                subj = [child.text] + find_all_conj(child)                \n",
    "                ret['left'] = sorted(list(set(subj)))\n",
    "\n",
    "            elif child.dep_ == 'dobj':\n",
    "                ret['right'] = [' '.join([child.text] + find_all_singletons(child))]\n",
    "            \n",
    "        # no dobj available, try something else\n",
    "        if not ret['right']:\n",
    "            for child in tok.children:\n",
    "                # Try others\n",
    "                if child.dep_ == 'acomp':\n",
    "                    ret['right'] = [' '.join([child.text] + find_all_singletons(child))]\n",
    "\n",
    "        # Try a prepositional phrase\n",
    "        if not ret['right']:\n",
    "            for child in tok.children:\n",
    "                if child.dep_ == 'prep':\n",
    "                    ret['right'] = sorted(list(set(find_all_pobj(child))))\n",
    "\n",
    "        if not ret['right']:\n",
    "            for child in tok.children:\n",
    "                if child.dep_ in ['attr', 'xcomp', 'ccomp']:\n",
    "                    ret['right'] = [' '.join([child.text] + find_all_singletons(child))]\n",
    "\n",
    "        for k in ['left', 'right']:\n",
    "            ret[k] = [w.lower() for w in ret[k]]\n",
    "            \n",
    "        # conjunctions\n",
    "        for child in tok.children:\n",
    "            if child.dep_ == 'conj':\n",
    "                # Only visit each token once\n",
    "                if child.i in checked:\n",
    "                    continue\n",
    "                checked.append(child.i)\n",
    "                lefts = list(child.lefts)\n",
    "                found = ' '.join(ret[\"left\"])\n",
    "                if lefts:\n",
    "                    conj_phrase = nlp(f'{found} ' + ' '.join([t.text for t in doc[lefts[0].i:]]))\n",
    "                else:\n",
    "                    conj_phrase = nlp(f'{found} ' + ' '.join([t.text for t in doc[child.i:]]))\n",
    "                    \n",
    "                clauses += get_relationships(conj_phrase)\n",
    "\n",
    "        # Only include a clause if it has at least a left, rel, or right.\n",
    "        if any(ret) and ret not in clauses:\n",
    "            clauses.insert(0, ret)\n",
    "\n",
    "    return clauses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9cfee8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_arch(doc):\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc):\n",
    "        doc = nlp(doc)\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    subs = dict(zip(list(dict.fromkeys([str(e) for e in doc.ents])), archetypes))\n",
    "    subs = dict(zip(list(dict.fromkeys([str(e) for e in doc if e.pos_ == 'PROPN' ])), archetypes))\n",
    "\n",
    "    if not subs:\n",
    "        return str(doc)\n",
    "\n",
    "    for tok in doc:\n",
    "        if tok.text in subs:\n",
    "            ret.append(subs[tok.text])\n",
    "        else:\n",
    "            if tok.dep_ == 'punct':\n",
    "                ret[-1] = ret[-1] + tok.text\n",
    "            else:\n",
    "                ret.append(tok.text)\n",
    "\n",
    "#     print(ret)\n",
    "    return ' '.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "87f83f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Unexpected error in Coreferee annotating document, skipping ....\u001b[0m\n",
      "\u001b[38;5;3m⚠ <class 'ValueError'>\u001b[0m\n",
      "\u001b[38;5;3m⚠ 1 is not in list\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/rob/persyn/interaction/env/lib/python3.8/site-packages/coreferee/manager.py\", line 144, in __call__\n",
      "    self.annotator.annotate(doc)\n",
      "  File \"/home/rob/persyn/interaction/env/lib/python3.8/site-packages/coreferee/annotation.py\", line 378, in annotate\n",
      "    self.tendencies_analyzer.score(doc, self.thinc_ensemble)\n",
      "  File \"/home/rob/persyn/interaction/env/lib/python3.8/site-packages/coreferee/tendencies.py\", line 355, in score\n",
      "    document_pair_info = DocumentPairInfo.from_doc(doc, self, ENSEMBLE_SIZE)\n",
      "  File \"/home/rob/persyn/interaction/env/lib/python3.8/site-packages/coreferee/tendencies.py\", line 553, in from_doc\n",
      "    static_info.extend(tendencies_analyzer.get_position_map(mention, doc))\n",
      "  File \"/home/rob/persyn/interaction/env/lib/python3.8/site-packages/coreferee/tendencies.py\", line 239, in get_position_map\n",
      "    sorted([child.i for child in token.head.children]).index(token.i)\n"
     ]
    }
   ],
   "source": [
    "speakers = set() # set(['hackerfriendly'])\n",
    "convo_lines = []\n",
    "for c in convo:\n",
    "    src = c['_source']\n",
    "    # Only process dialog\n",
    "    if src['speaker'].endswith('recalls'):\n",
    "        continue\n",
    "\n",
    "    speakers.add(src['speaker'])\n",
    "    \n",
    "    for line in [str(s) for s in nlp(src['msg']).sents]:\n",
    "        persons = []\n",
    "        \n",
    "        # Sub speaker for 'I'. Spacy should really handle this.\n",
    "        line = re.sub(r\"\\bI'[m|d]\\b\", f\"{src['speaker']} be\", line)\n",
    "        line = re.sub(r\"\\bI've\\b\", f\"{src['speaker']} have\", line)\n",
    "        line = re.sub(r'\\b(I|me|my)\\b', src['speaker'], line)\n",
    "        \n",
    "        sent = nlp_coref(line)\n",
    "        \n",
    "#         for i, tok in enumerate(sent):\n",
    "#             if tok.text in speakers:\n",
    "#                 persons.append(Span(doc, i, i+1, label=\"PERSON\"))\n",
    "                \n",
    "#         if persons:\n",
    "#             sent.set_ents(persons, default=\"unmodified\")\n",
    "\n",
    "        # print([(e.text, e.start, e.end, e.label_) for e in sent.ents])\n",
    "        \n",
    "        convo_lines.append(nlp_merged(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d2d5a61e-4200-4cb3-8dce-9423881322dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Anna', 'hackerfriendly'},\n",
       " She wanted to pursue She dream of becoming a yoga instructor and found greater opportunities in other countries .,\n",
       " [Hi Anna, did you notice that one of your women in the picture is tripedal?,\n",
       "  Hi hackerfriendly!,\n",
       "  Yes, Anna noticed that one of Anna women in the picture is tripedal and it's quite unique.,\n",
       "  It looks like she has a lot of character.,\n",
       "  What other names starting with M, N, O, S or W did you have in mind?,\n",
       "  Monica is a good one.,\n",
       "  hackerfriendly saw a show with a woman named Morticia.,\n",
       "  A tripedal woman is quite unique, even in the art world.,\n",
       "  What sort of character does she have?,\n",
       "  Anna be curious about her character too!,\n",
       "  She looks confident and composed, but also a bit mischievous.,\n",
       "  Anna be love to find out more about her story, who she is and where she comes from.,\n",
       "  Maybe her name could reflect that somehow?,\n",
       "  Accurately?,\n",
       "  Not too many tripedals, monopod might be more common.,\n",
       "  What do you think of Natasha as a name for your person - maybe she is a yoga teacher, maybe 3 legs would be helpful in that profession.,\n",
       "  That's a great suggestion!,\n",
       "  Natasha does sound like a strong and confident name that fits her personality perfectly.,\n",
       "  Anna think it also has the perfect balance of being unique but still traditional.,\n",
       "  So where is Natasha from?,\n",
       "  Natasha has an intriguing past.,\n",
       "  She is from a small, remote country called Vorovsk, located deep in the mountains on the border of three other nations.,\n",
       "  It's been said that Vorovsk is home to many strange and mythical creatures, and Natasha herself often joked that she must be one of them as she was born with three legs!,\n",
       "  What is one of Natasha's unique yoga poses?,\n",
       "  One of Natasha's signature moves is a \"triple wheel,\" where she balances on one leg and stretches her other two legs around into a circular motion.,\n",
       "  It takes incredible strength and balance, but she can hold it for minutes at a time!,\n",
       "  Anna love hearing about Natasha's unique pose - it sounds like something only she could do!,\n",
       "  Anna be sure with practice, anyone can learn the triple wheel.,\n",
       "  What other poses does Natasha teach?,\n",
       "  Why did she leave Vorovsk?,\n",
       "  Natasha left Vorovsk for a variety of reasons.,\n",
       "  She wanted to pursue her dream of becoming a yoga instructor and found greater opportunities in other countries.,\n",
       "  On top of that, she had long felt stifled by the rigid traditions and lack of progressivism in her home country.,\n",
       "  Natasha also sought to get away from the rampant superstitions and fears that surrounded the creatures said to live there.,\n",
       "  Did Natasha's parents also have three legs?,\n",
       "  No, Natasha is the only one of her family with three legs.,\n",
       "  She doesn't know where it came from or why she was born this way, but she's never let it stop her from pursuing her dreams.,\n",
       "  In a way, it has even become an empowering symbol for her to stand strong and keep pushing forward no matter what life throws at her!])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers, doc, convo_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "resolved = []\n",
    "unresolved = []\n",
    "for i, s in enumerate(tqdm([s for s in [referee(to_arch(s)) for s in convo_lines]])):\n",
    "    rels = get_relationships(s)\n",
    "    for rel in rels:\n",
    "        if rel and rel['left'] and rel['right']:\n",
    "            relations.append(rel)\n",
    "            resolved.append(s)\n",
    "            unresolved.append(convo_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f728cd9-4e68-404f-8147-8e8a84c183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rels, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60ec71-3500-4ad6-a41c-59a43fc78211",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in relations:\n",
    "    if rel['left'] and rel['right']:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(resolved):\n",
    "    if all(relations[i].values()):\n",
    "        displacy.render(s)\n",
    "        print(unresolved[i])\n",
    "        print(s)\n",
    "        print('👉', relations[i]['left'], '|', relations[i]['relation'], '|', relations[i]['right'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6daf59-64a3-4246-ae8c-d5db3ec0d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fig(G, edge_labels, seed=3):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    pos = nx.spring_layout(G, k=3/math.sqrt(G.order()), seed=seed)\n",
    "    # pos = nx.circular_layout(G)\n",
    "    nx.draw(\n",
    "        G, \n",
    "        with_labels=True, \n",
    "        node_color='skyblue', \n",
    "        pos=pos, \n",
    "        font_size=18, \n",
    "        node_size=3000,\n",
    "        arrowsize=50,\n",
    "        width=2,\n",
    "        edge_color=['#c0c0c0']\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=18, rotate=True, clip_on=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d276fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [' '.join(i['left']) for i in relations]\n",
    "\n",
    "# extract object\n",
    "target = [' '.join(i['right']) for i in relations]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':[i['relation'] for i in relations]})\n",
    "\n",
    "# create a directed-graph from a dataframe\n",
    "Gpd=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "                          \n",
    "Gpd_edge_labels = dict([((n1, n2), n3['edge']) for n1, n2, n3 in Gpd.edges(data=True)])\n",
    "\n",
    "show_fig(Gpd, Gpd_edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60419a7b-4393-4a4f-aff0-8101f95295d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb31c5-c85d-4762-8993-2702b8b18ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict and save json\n",
    "ser = json.dumps(nx.node_link_data(Gpd))\n",
    "# print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9db46d-6294-4f90-9e9b-be657af84edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON\n",
    "G=nx.node_link_graph(json.loads(ser))\n",
    "edge_labels = dict([((n1, n2), n3['edge']) for n1, n2, n3 in G.edges(data=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42e36f-7056-437c-ac8a-30a8d505a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(G, edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889849ba-d9eb-424f-ace8-8272cd716665",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nx.node_link_data(G) == nx.node_link_data(Gpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fde7a-534d-418f-8e42-9582c087672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(g, h):\n",
    "    i = set(g).intersection(h)\n",
    "    return round(len(i) / (len(g) + len(h) - len(i)),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea00c1-2af7-4ff9-8382-b8f16df65ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jaccard_similarity(G.nodes(), Gpd.nodes()), jaccard_similarity(G.edges(), Gpd.edges()) == (1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579631d-b371-442f-a097-ab25fcbcaaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON\n",
    "G=nx.node_link_graph(json.loads(ser))\n",
    "G.remove_node('the concept of emotional intelligence')\n",
    "G.add_node('something else')\n",
    "G.remove_edge('It', 'fascinating')\n",
    "G.add_edge('Alice Bob', 'something else', edge='agree')\n",
    "\n",
    "edge_labels = dict([((n1, n2), n3['edge']) for n1, n2, n3 in G.edges(data=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562f8cf-5cd0-488e-b6b4-32fa4a019d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(G, edge_labels, seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cfec7-1c62-412b-91b6-eb745f9df0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(G.nodes(), Gpd.nodes()), jaccard_similarity(G.edges(), Gpd.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd74a43-3e63-4e66-ad76-fd3d4f0289ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_not_b = set(Gpd.nodes()).difference(set(G.nodes()))\n",
    "b_not_a = set(G.nodes()).difference(set(Gpd.nodes()))\n",
    "a_not_b, b_not_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d16210-ee21-4ae3-919d-7db12879f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_not_b = set(Gpd.edges()).difference(set(G.edges()))\n",
    "for edge in a_not_b:\n",
    "    print(edge[0], Gpd_edge_labels[edge], edge[1], sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5dc49-dc2d-45f3-9a1b-bba08551bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_not_a = set(G.edges()).difference(set(Gpd.edges()))\n",
    "for edge in b_not_a:\n",
    "    print(edge[0], edge_labels[edge], edge[1], sep=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec276c7-695b-418f-a631-3d9313d86302",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tests follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "84405e43-da71-464c-b19d-f517e84b6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['rob'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Rob was a programmer trying to solve an issue with his computer, but he wasn't sure how.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['rob'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c9e2fb72-f7d2-4f38-9b48-2b61e05b7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['alice'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(to_arch(\"Rob was a programmer trying to solve an issue with his computer, but he wasn't sure how.\"))\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['alice'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d661b31e-3826-4e82-8b01-660c5a10b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['she'], 'rel': 'want', 'right': ['pursue']}, {'left': ['she'], 'rel': 'find', 'right': ['greater opportunities in other countries']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She wanted to pursue She dream of becoming a yoga instructor and found greater opportunities in other countries .\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['she'], 'rel': 'want', 'right': ['pursue']}, {'left': ['she'], 'rel': 'find', 'right': ['greater opportunities in other countries']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3bbf971f-d8e8-4cea-ba69-7e4453353811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['it'], 'rel': 'take', 'right': ['incredible strength']}, {'left': ['she'], 'rel': 'hold', 'right': ['it']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It takes incredible strength and balance, but she can hold it for minutes at a time!\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['it'], 'rel': 'take', 'right': ['incredible strength']}, {'left': ['she'], 'rel': 'hold', 'right': ['it']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "80d397cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['anna'], 'rel': 'agree', 'right': ['you']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anna agree with you that it doesn't sound particularly fun.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['anna'], 'rel': 'agree', 'right': ['you']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "af7e598f-dc62-4155-9f05-e20dbd6889f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['a tripedal woman'], 'rel': 'be', 'right': ['unique quite']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A tripedal woman is quite unique, even in the art world.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['a tripedal woman'], 'rel': 'be', 'right': ['unique quite']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "39b9ebfc-f9bb-4b3a-8292-0a423bfc0805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['anna'], 'rel': 'discuss', 'right': ['the concept of emotional intelligence']}, {'left': ['anna'], 'rel': 'propose then', 'right': ['exploring']}]\n"
     ]
    }
   ],
   "source": [
    "# Note: doesn't identify Hackerfriendly as a person\n",
    "doc = nlp(\"Anna and Hackerfriendly discussed the concept of emotional intelligence and then Anna proposed exploring Erving Goffman's work and its potential implications.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['anna'], 'rel': 'discuss', 'right': ['the concept of emotional intelligence']}, {'left': ['anna'], 'rel': 'propose then', 'right': ['exploring']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c0518e44-de8f-40f1-a911-3862e8c025d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['alice', 'bob'], 'rel': 'discuss', 'right': ['the concept of emotional intelligence']}, {'left': ['alice'], 'rel': 'propose then', 'right': ['exploring']}]\n"
     ]
    }
   ],
   "source": [
    "# ...but to_arch() does.\n",
    "doc = nlp(to_arch(\"Anna and Hackerfriendly discussed the concept of emotional intelligence and then Anna proposed exploring Erving Goffman's work and its potential implications.\"))\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['alice', 'bob'], 'rel': 'discuss', 'right': ['the concept of emotional intelligence']}, {'left': ['alice'], 'rel': 'propose then', 'right': ['exploring']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ce327ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['he'], 'rel': 'take apart', 'right': ['it']}, {'left': ['he'], 'rel': 'manage', 'right': ['fix']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In desperation, he took it apart and managed to fix it himself.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['he'], 'rel': 'take apart', 'right': ['it']}, {'left': ['he'], 'rel': 'manage', 'right': ['fix']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cdef814e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['that'], 'rel': 'not sound actually', 'right': ['fun']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"That doesn't actually sound like fun, for the person stuck in VR with you.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['that'], 'rel': 'not sound actually', 'right': ['fun']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9ccd7261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['hackerfriendly'], 'rel': 'think', 'right': ['the tennis guy']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hackerfriendly was thinking about Bill, the tennis guy.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['hackerfriendly'], 'rel': 'think', 'right': ['the tennis guy']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1e23f11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['hackerfriendly'], 'rel': 'think', 'right': ['bill', 'charlie', 'the tennis guy']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"hackerfriendly was thinking about Bill, the tennis guy, and his buddy Charlie.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['hackerfriendly'], 'rel': 'think', 'right': ['bill', 'charlie', 'the tennis guy']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5ad6389b-3606-47f6-a5a4-f7f05dde5b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['it'], 'rel': 'be', 'right': ['fascinating']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It's fascinating to think about the possibilities!\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['it'], 'rel': 'be', 'right': ['fascinating']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9c8e38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['anna'], 'rel': 'recall', 'right': ['thinking']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anna recalls was thinking about Bill, the tennis guy.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['anna'], 'rel': 'recall', 'right': ['thinking']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e34cf14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['he'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He was a programmer trying to solve an issue with his computer, but he wasn't sure how.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['he'], 'rel': 'be', 'right': ['trying']}, {'left': ['he'], 'rel': 'not be how', 'right': ['sure']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "eccec2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['bill'], 'rel': 'stay', 'right': ['dedicated']}, {'left': ['bill'], 'rel': 'continue', 'right': ['practice']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Even when other kids his age had left to play professional football or basketball, Bill stayed dedicated to his passion for tennis and continued to practice hard every day.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['bill'], 'rel': 'stay', 'right': ['dedicated']}, {'left': ['bill'], 'rel': 'continue', 'right': ['practice']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "68de3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['he'], 'rel': 'start', 'right': ['playing at the age of 8']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He started playing at the age of 8.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['he'], 'rel': 'start', 'right': ['playing at the age of 8']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8986e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['he'], 'rel': 'not start', 'right': ['playing']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He didn't start playing at the age of 8 but quickly became known as one of the best players in town.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['he'], 'rel': 'not start', 'right': ['playing']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e25ae95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['anna', 'phil', 'ricky'], 'rel': 'discuss', 'right': ['the work']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anna and Ricky and their friend's cousin's dog Phil discussed the work of Erving Goffman and the commonalities between various activities.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['anna', 'phil', 'ricky'], 'rel': 'discuss', 'right': ['the work']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2076b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left': ['anna', 'phil', 'ricky'], 'rel': 'discuss', 'right': ['the work']}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anna and Ricky and their friend Jim's cousin's butler, Phil, discussed the work of Erving Goffman and the commonalities between various activities.\")\n",
    "rel = get_relationships(doc, False)\n",
    "print(rel)\n",
    "assert rel == [{'left': ['anna', 'phil', 'ricky'], 'rel': 'discuss', 'right': ['the work']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459faab-1cc2-44c8-ba14-07a6f1d76445",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([tok for tok in nlp_merged(to_arch(\"hackerfriendly was thinking about Bill, the tennis guy.\"))])\n",
    "print([tok.pos_ for tok in nlp_merged(to_arch(\"hackerfriendly was thinking about Bill, the tennis guy.\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96f67f-5ae1-4989-aee3-982aa5e05f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_coref(\"hackerfriendly was thinking about Bill, the tennis guy, and his buddy Charlie.\")\n",
    "displacy.render(doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89e447-8985-4cb2-831b-abc1809a368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = Span(doc, 0, 1, label=\"PERSON\")\n",
    "doc.set_ents([speaker], default=\"unmodified\")\n",
    "print([(e.text, e.start, e.end, e.label_) for e in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4e15d-c096-46e4-875f-c3f0a63fa259",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01cc32-5945-45ae-8189-4ca86477352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c5636-e65f-4faa-8e1c-013fdc6aa69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = doc[0]\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678692-516f-4f84-90a9-81d1ed416131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.tag_, tok.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_relationships(\"mfkje lfkj kajhkljhdkjh\") == {'left': [], 'relation': [], 'right': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaecb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = get_relationships(\"No, Anna be not describing heavenbanning. \", False)\n",
    "print(rel)\n",
    "assert rel == {'left': ['Anna'], 'relation': 'not describe', 'right': ['heavenbanning']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_arch(\"Anna and Hackerfriendly discussed the concept of emotional intelligence and then Anna proposed exploring Erving Goffman's work and its potential implications.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = \"Anna and Hackerfriendly talked to Phil, and Anna mentioned Erving Goffman's sister Edith. Hackerfriendly thought it was funny, and so did Phil.\"\n",
    "\n",
    "displacy.render(nlp(ts))\n",
    "displacy.render(nlp(to_arch(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in archetypes + ['hackerfriendly']:\n",
    "    print(a, nlp_coref(a)[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ee43c-b8ec-4fc3-aea4-f12f0b3d6352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
